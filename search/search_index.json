{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AudioZEN","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"concepts/experiment_arguments/","title":"Experiment arguments","text":"<p>AudioZEN uses TOML configuration files (<code>*.toml</code>) to configure and manage experiments. Each experiment is configured by a <code>*.toml</code> file, which contains the experiment meta information, trainer, loss function, learning rate scheduler, optimizer, model, dataset, and acoustic features. the basename of the <code>*.toml</code> file is used as the experiment ID or identifier. You can track configuration changes using version control and reproduce experiments by using the same configuration file. For more information on TOML syntax, visit the TOML website.</p>"},{"location":"concepts/experiment_arguments/#sample-toml-file","title":"Sample <code>*.toml</code> file","text":"<p>This sample file demonstrates many settings available for configuration in AudioZEN.</p> <pre><code>[meta]\nsave_dir = \"sdnn_delays/exp\"\nseed = 0\nuse_amp = false\nuse_deterministic_algorithms = false\n[trainer]\npath = \"trainer.Trainer\"\n[trainer.args]\nmax_epoch = 9999\nclip_grad_norm_value = 5\n[acoustics]\nn_fft = 512\nwin_length = 256\nsr = 16000\nhop_length = 256\n[loss]\npath = \"audiozen.loss.SoftDTWLoss\"\n[loss.args]\ngamma = 0.1\n[optimizer]\npath = \"torch.optim.RAdam\"\n[optimizer.args]\nlr = 0.01\nweight_decay = 1e-5\n[model]\npath = \"model.Model\"\n[model.args]\nthreshold = 0.1\ntau_grad = 0.1\nscale_grad = 0.8\nmax_delay = 64\nout_delay = 0\n</code></pre> <p>Check any experiment configuration file in the <code>recipes</code> directory for more details.</p>"},{"location":"concepts/experiment_arguments/#configuration-details","title":"Configuration details","text":"<p>In the audiozen configuration file, we must contain the following sections:</p> <ul> <li><code>meta</code>: Configure the experiment meta information, such as <code>save_dir</code>, <code>seed</code>, etc.</li> <li><code>trainer</code>: Configure the trainer.</li> <li><code>loss_function</code>: Configure the loss function.</li> <li><code>lr_scheduler</code>: Configure the learning rate scheduler.</li> <li><code>optimizer</code>: Configure the optimizer.</li> <li><code>model</code>: Configure the model.</li> <li><code>dataset</code>: Configure the dataset.</li> <li><code>acoustics</code>: Configure the acoustic features.</li> </ul>"},{"location":"concepts/experiment_arguments/#meta","title":"<code>meta</code>","text":"<p>The <code>meta</code> section is used to configure the experiment meta information.</p> Item Description <code>save_dir</code> The directory where the experiment is saved. The log information, model checkpoints, and enhanced audio files will be stored in this directory. <code>seed</code> The random seed used to initialize the random number generator. <code>use_amp</code> Whether to use automatic mixed precision (AMP) to accelerate the training. <code>use_deterministic_algorithms</code> Whether to use nondeterministic algorithms to accelerate the training. If it is True, the training will be slower but more reproducible."},{"location":"concepts/experiment_arguments/#trainer","title":"<code>trainer</code>","text":"<p>The <code>trainer</code> section is used to configure the trainer. It contains two parts: <code>path</code> and <code>args</code>. <code>path</code> is a string that specifies the path to the trainer class. <code>args</code> is a dictionary that specifies the arguments of the trainer class. It looks like:</p> <pre><code>[trainer]\npath = \"trainer.Trainer\"\n[trainer.args]\nmax_epochs = 100\nclip_grad_norm_value = 5\n...\n</code></pre> <p>In this case, AudioZEN will load the <code>Trainer</code> class from <code>trainer.py</code> and initialize it with the arguments in the <code>[trainer.args]</code> section. <code>Trainer</code> class must be a subclass of <code>audiozen.trainer.base_trainer.BaseTrainer</code>. It supports the following arguments at least:</p> Item Description <code>max_epochs</code> The maximum number of epochs to train. <code>clip_grad_norm_value</code> The maximum norm of the gradients used for clipping. <code>save_max_score</code> Whether to find the best model by the maximum score. <code>save_ckpt_interval</code> The interval of saving checkpoints. <code>patience</code> The number of epochs with no improvement after which the training will be stopped. <code>validation_interval</code> The interval of validation. <code>max_num_checkpoints</code> The maximum number of checkpoints to keep. Saving too many checkpoints causes disk space to run out."},{"location":"concepts/experiment_arguments/#finding-modules-by-path-argument","title":"Finding modules by <code>path</code> argument","text":"<p>We support multiple ways to find modules by the <code>path</code> argument in the <code>*.toml</code>. For example, we have the following directory structure:</p> <pre><code>recipes/intel_ndns_challenge\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 run.py\n\u2514\u2500\u2500 sdnn_intel_ndns_challengedelays\n    \u251c\u2500\u2500 baseline.toml\n    \u251c\u2500\u2500 exp\n    \u2502   \u2514\u2500\u2500 baseline\n    \u2502       \u2514\u2500\u2500 baseline.log\n    \u251c\u2500\u2500 model.py\n    \u2514\u2500\u2500 trainer.py\n</code></pre> <pre><code>sys.path = [\n'/path/to/audiozen/recipes/intel_ndns_challenge/sdnn_delays',\n'/path/to/audiozen/recipes/intel_ndns_challenge',\n...\n]\n</code></pre> <p>In <code>recipes/dns_1/baseline.toml</code>, the <code>path</code> of the <code>trainer</code> is set to:</p> <pre><code>[trainer]\npath = \"trainer.Trainer\"\n</code></pre> <p>In this case, we will try to find the <code>Trainer</code> class in <code>recipes/dns_1/trainer</code>. If we set the <code>path</code> to:</p> <pre><code>[trainer]\npath = \"audiozen.trainer.custom_trainer.CustomTrainer\"\n</code></pre> <p>We will try to find the <code>CustomTrainer</code> class in <code>audiozen/trainer/custom_trainer.py</code>.</p> <p>Note</p> <p>If you want to call <code>Trainer</code> in <code>audiozen</code> package, you must install it in editable way by <code>pip install -e .</code> first.</p>"},{"location":"concepts/experiment_arguments/#loss_function-lr_scheduler-optimizer-model-and-dataset","title":"<code>loss_function</code>, <code>lr_scheduler</code>, <code>optimizer</code>, <code>model</code>, and <code>dataset</code>","text":"<p><code>loss_function</code>, <code>lr_scheduler</code>, <code>optimizer</code>, <code>model</code>, <code>dataset</code> sections are used to configure the loss function, learning rate scheduler, optimizer, model, and dataset, respectively. They have the same logic as the <code>trainer</code> section.</p> <pre><code>[loss_function|lr_scheduler|optimizer|model|dataset]\npath = \"...\"\n[loss_function|lr_scheduler|optimizer|model|dataset.args]\n...\n</code></pre> <p>For example, you may use the loss function provided by PyTorch or implement your own loss function. For example, the following configuration is used to configure the <code>MSELoss</code>:</p> <pre><code>[loss_function]\npath = \"torch.nn.MSELoss\"\n[loss_function.args]\n</code></pre> <p>Configuration of a custom loss function:</p> <pre><code>[loss_function]\npath = \"audiozen.loss.MyLoss\"\n[loss_function.args]\nweights = [1.0, 1.0]\n...\n</code></pre> <p>Note</p> <p>You must keep the <code>[loss_function.args]</code> section even this loss function does not need any arguments.</p> <p>You may use the learning rate scheduler provided by PyTorch or implement your own learning rate scheduler. For example, the following configuration is used to configure the <code>StepLR</code>:</p> <pre><code>[lr_scheduler]\npath = \"torch.optim.lr_scheduler.StepLR\"\n[lr_scheduler.args]\nstep_size = 100\ngamma = 0.5\n</code></pre>"},{"location":"concepts/experiment_arguments/#acoustics","title":"<code>acoustics</code>","text":"<p>The <code>acoustics</code> section is used to configure the acoustic features. These configurations are used for the whole project, like visualization, except for the <code>dataloader</code> and <code>model</code> sections.</p> Item Description <code>sr</code> The sample rate of the audio. <code>n_fft</code> The number of FFT points. <code>hop_length</code> The number of samples between successive frames. <code>win_length</code> The length of the STFT window."},{"location":"concepts/trainer/","title":"Trainer","text":"<p>For each experiment, we need to define a custom trainer to train the model. The custom trainer must inherit from <code>audiozen.trainer.base_trainer.BaseTrainer</code> and implement the following methods:</p> <ul> <li><code>training_step</code>: The training step. It contains the operations to be executed in each training iteration.</li> <li><code>training_epoch_end</code>: The training epoch end. It contains the operations to be executed at the end of each training epoch.</li> <li><code>validation_step</code>: The validation step. It contains the operations to be executed in each validation iteration.</li> <li><code>validation_epoch_end</code>: The validation epoch end. It contains the operations to be executed at the end of each validation epoch.</li> <li><code>test_step</code>: The test step. It contains the operations to be executed in each test iteration.</li> <li><code>test_epoch_end</code>: The test epoch end. It contains the operations to be executed at the end of each test epoch.</li> </ul>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Implement a training step.</p> <p>Implement your own training step here. The input batch is from a training dataloader and the output of this function should be a loss tensor. Here is the persuade code for training a model:</p> <pre><code>for epoch in range(start_epoch, end_epoch):\nself.model.train()\ntraining_epoch_output = []\nfor batch, batch_index in dataloader:\nzero_grad()\nloss = training_step(batch, batch_idx)\nloss.backward()\noptimizer.step()\ntraining_epoch_output.append(loss)\ntraining_epoch_end(training_epoch_output)\nsave_checkpoint()\nif some_condition:\nscore = validate()\nif score &gt; best_score:\nsave_checkpoint(best=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>a batch of data, which passed from a custom training dataloader.</p> required <code>batch_idx</code> <p>the index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <p>the loss of the batch.</p>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Implement a training step.</p> <p>Implement your own training step here. The input batch is from a training dataloader and the output of this function should be a loss tensor. Here is the persuade code for training a model:</p> <pre><code>for epoch in range(start_epoch, end_epoch):\nself.model.train()\ntraining_epoch_output = []\nfor batch, batch_index in dataloader:\nzero_grad()\nloss = training_step(batch, batch_idx)\nloss.backward()\noptimizer.step()\ntraining_epoch_output.append(loss)\ntraining_epoch_end(training_epoch_output)\nsave_checkpoint()\nif some_condition:\nscore = validate()\nif score &gt; best_score:\nsave_checkpoint(best=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>a batch of data, which passed from a custom training dataloader.</p> required <code>batch_idx</code> <p>the index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <p>the loss of the batch.</p>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.training_epoch_end","title":"<code>training_epoch_end(training_epoch_output)</code>","text":"<p>Implement the logic of the end of a training epoch.</p> <p>When the training epoch ends, this function will be called. The input is a list of the loss value of each batch in the training epoch. You may want to log the epoch-level training loss here.</p> <pre><code>for epoch in range(start_epoch, end_epoch):\nself.model.train()\ntraining_epoch_output = []\nfor batch, batch_index in dataloader:\nzero_grad()\nloss = training_step(batch, batch_idx)\nloss.backward()\noptimizer.step()\ntraining_epoch_output.append(loss)\ntraining_epoch_end(training_epoch_output)\nsave_checkpoint()\nif some_condition:\nscore = validate()\nif score &gt; best_score:\nsave_checkpoint(best=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>training_epoch_output</code> <p>the output of the training epoch. It may a list of the output of each batch.</p> required"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Implement a validation step.</p> <p>This function defines the validation step. The input batch is from a validation dataloader. Here is the persuade code for validating a model:</p> <pre><code>validation_output = []\nfor dataloader_idx, dataloader in dataloaders:\nfor batch_index, batch in dataloader:\nloss_or_data = validation_step(batch, batch_idx)\nvalidation_epoch_output.append(loss_or_data)\nscore = validation_epoch_end(validation_epoch_output)\nreturn score\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>a batch of data.</p> required <code>batch_idx</code> <p>the index of the batch.</p> required <code>dataloader_idx</code> <p>the index of the dataloader.</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>the output of the batch. It may enhanced audio signals.</p>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.validation_epoch_end","title":"<code>validation_epoch_end(validation_epoch_output)</code>","text":"<p>Validation epoch end.</p> <p>The input <code>validation_epoch_output</code> will be a list of list. For example, if you have two dataloaders, the <code>validation_epoch_output</code> will be:</p> <pre><code>validation_epoch_output = [\n[dataloader_1_batch_1_output, dataloader_1_batch_2_output, ...],\n[dataloader_2_batch_1_output, dataloader_2_batch_2_output, ...],\n...\n]\n</code></pre> <p>The output of this function should be a metric score, which will be used to determine whether the current model is the best model.</p> <pre><code>validation_output = []\nfor dataloader_idx, dataloader in dataloaders:\nfor batch_index, batch in dataloader:\nloss_or_data = validation_step(batch, batch_idx)\nvalidation_epoch_output.append(loss_or_data)\nscore = validation_epoch_end(validation_epoch_output)\nreturn score\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>validation_epoch_output</code> <p>the output of the validation epoch. It is a list of list.</p> required <p>Returns:</p> Name Type Description <code>score</code> <p>the metric score of the validation epoch.</p>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Similar to validation_step, but for testing.</p> <pre><code>load_checkpoint(ckpt_path)\nfor batch, batch_index in dataloader:\nloss = test_step(batch, batch_idx)\ntest_epoch_output.append(loss)\ntest_epoch_end(test_epoch_output)\nreturn score\n</code></pre>"},{"location":"concepts/trainer/#audiozen.trainer.base_trainer.BaseTrainer.test_epoch_end","title":"<code>test_epoch_end(test_epoch_output)</code>","text":"<p>Similar to validation_epoch_end, but for testing.</p> <pre><code>load_checkpoint(ckpt_path)\nfor batch, batch_index in dataloader:\nloss = test_step(batch, batch_idx)\ntest_epoch_output.append(loss)\ntest_epoch_end(test_epoch_output)\nreturn score\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<p>AudioZEN package is built on top of PyTorch and provides standard audio signal processing and deep learning tools. To install the PyTorch binaries, you will need to use at least one of two supported package managers: Anaconda (or Miniconda) and pip. Anaconda (or Miniconda) is the recommended package manager as it will provide you all of the PyTorch dependencies in one, sandboxed install, including Python and pip. For GPU parallel training, CUDA (version 10.2 or higher) and the corresponding CuDNN acceleration library must be installed.</p>"},{"location":"getting_started/installation/#installation_1","title":"Installation","text":""},{"location":"getting_started/installation/#create-virtual-environment","title":"Create virtual environment","text":"<p>First, create a Conda virtual environment with Python. Here, <code>python=3.10</code> is tested, but you may also use other versions.</p> <pre><code>conda create --name audiozen python=3.10\nconda activate audiozen\n</code></pre> <p>The following steps will assume you have activated the <code>audiozen</code> environment.</p>"},{"location":"getting_started/installation/#install-conda-dependencies","title":"Install Conda dependencies","text":"<p>Some dependencies of AudioZEN, like PyTorch, Lava, and Tensorboard, are recommended to be installed using Conda instead of PyPI. First, we install a CUDA-capable PyTorch. Although <code>pytorch=1.12.1</code> has been tested, you may also use other versions. We use <code>CUDA 11.6</code> as an example:</p> <pre><code>conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge\n</code></pre> <p>Install Lava and Lava-dl, which are used for neuromorphic computing. Check Lava and Lava-dl for more details.</p> <pre><code>conda install lava lava-dl -c conda-forge\n</code></pre> <p>Install other Conda dependencies:</p> <pre><code>conda install tensorboard joblib matplotlib\n\n# (Optional) If you have \"mp3\" format data in your dataset, install ffmpeg first.\nconda install -c conda-forge ffmpeg\n</code></pre>"},{"location":"getting_started/installation/#install-pypi-dependencies","title":"Install PyPI dependencies","text":"<p>Clone the repository and install PyPI dependencies via <code>pip -r requirements.txt</code>. Check <code>requirements.txt</code> for more details.</p> <pre><code>git clone git@github.com:haoxiangsnr/audiozen.git\n\ncd audiozen\n\npip install -r requirements.txt\n</code></pre>"},{"location":"getting_started/installation/#install-audiozen-package-in-editable-mode","title":"Install AudioZEN package in editable mode","text":"<p>Finally, we will install the AudioZEN package in editable mode (a.k.a. development mode). By installing in editable mode, we can call <code>audiozen</code> package in everywhere of code, e.g, in <code>recipes</code> and <code>tools</code> folders. In addition, we can modify the source code of <code>audiozen</code> package directly. Any changes to the original package would reflect directly in your environment.</p> <pre><code>pip install --editable .\n</code></pre> <p>Ok, all installations have done.</p>"},{"location":"getting_started/installation/#references","title":"References","text":"<ul> <li>Speed up your Conda installs with Mamba</li> <li>Use the THU Anaconda mirror site to speed up the Conda installation.</li> <li>Use the THU PyPi mirror site to install PyPI dependencies.</li> </ul>"},{"location":"getting_started/logging_and_visualization/","title":"Logging and Visualization","text":"<p>After the training process is completed, the log information will be stored in the <code>save_dir</code> directory. Assuming that:</p> <ul> <li>The file name of the training configuration file is: <code>baseline.toml</code></li> <li>The value of the <code>save_dir</code> parameter in the training configuration file <code>basline.toml</code> is <code>~/exp</code></li> </ul> <p>Then the log information will be stored in the <code>~/exp/baseline</code> directory, which contains the following information:</p> <pre><code>.\n\u251c\u2500\u2500 baseline.log\n\u251c\u2500\u2500 checkpoints\n\u251c\u2500\u2500 config__2023_01_13--10_27_42.toml\n\u251c\u2500\u2500 enhanced\n\u2514\u2500\u2500 tb_log\n    \u2514\u2500\u2500 events.out.tfevents.1673576862.VM-97-67-ubuntu.3747605.0\n</code></pre> <ul> <li><code>baseline.log</code>: log information</li> <li><code>checkpoints/</code>: model checkpoints</li> <li><code>config__2023_04_13--10_27_42.toml</code>: training configuration file</li> <li><code>enhanced</code>: enhanced audio files when running in test mode</li> <li><code>tb_log/</code>: TensorBoard log information, we can visualize it through TensorBoard</li> </ul> <p>Currently, we only support TensorBoard for visualization. Assuming that the value of the <code>save_dir</code> parameter in the training configuration file <code>basline.toml</code> is <code>~/exp</code>, then we can use the following command to visualize the log information:</p> <pre><code>tensorboard --logdir ~/exp\n</code></pre>"},{"location":"getting_started/project_structure/","title":"Project Structure","text":"<p>Before going to details, let's take a look at the overall structure of the project. You may familiar with this project structure (<code>recipes/&lt;dataset&gt;/&lt;model&gt;</code>) if you have used ESPNet and SpeechBrain. AudioZEN is inspired by them, but it is more flexible and simpler.</p> <p>AudioZEN includes a core package and a series of recipes. The core package named <code>audiozen</code>, which provides common audio signal processing tools and deep learning trainers. As we have installed <code>audiozen</code> in editable mode, we can call <code>audiozen</code> package in everywhere of code. In addition, we can modify the source code of <code>audiozen</code> package directly. Any changes to the original package would reflect directly in your environment. For example, we can call <code>audiozen</code> package in <code>recipes</code> folder to train models on specific datasets and call <code>audiozen</code> package in <code>tools</code> folder to preprocess data. The recipes in the <code>recipes</code> folder are used to conduct the research on the audio/speech signal processing. Recipe introduced by Kaldi firstly. It provides a convenient and reproducible way to organize and save the deep learning training pipelines.</p> <p>The directory structure is as follows:</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 audiozen\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 acoustics\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 dataset\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 model\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 module\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 trainer\n\u251c\u2500\u2500 \ud83d\udcc1 docs\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 audiozen\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 acoustics\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 dataset\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 trainer\n\u251c\u2500\u2500 \ud83d\udcc1 notebooks\n\u251c\u2500\u2500 \ud83d\udcc1 recipes\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 dns_icassp_2020\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 cirm_lstm\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 baseline.toml\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 model.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 trainer.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 dataset_train.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 dataset_validation_dns_1.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 dataset_validation_dns_4_non_personalized.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 run.py\n\u2514\u2500\u2500 \ud83d\udcc1 tools\n</code></pre> <ul> <li>\ud83d\udcc1<code>audiozen/</code>: The core of the project. It contains the following subdirectories:<ul> <li>\ud83d\udcc1 <code>acoustics/</code>: Contain the code for audio signal processing.</li> <li>\ud83d\udcc1 <code>dataset/</code>: Contain the data loading and processing code.</li> <li>\ud83d\udcc1 <code>model/</code>: Contain the code for model definition and training.</li> <li>\ud83d\udcc1 <code>trainer/</code>: Contain the code for training and evaluation.</li> <li>...</li> </ul> </li> <li>\ud83d\udcc1 <code>docs/</code>: Contains the project's documentation.</li> <li>\ud83d\udcc1 <code>recipes/</code>: Contains the recipes for specific experiments.</li> <li>\ud83d\udcc1 <code>tools/</code>: Contains the code for additional tools, such as data preprocessing, model conversion, etc.</li> </ul> <p>In <code>recipes</code> folder, we name the subdirectory after the dataset. create a subdirectory for the dataset named after the model. For example, <code>recipes/dns_icassp_2020/</code> represents the dataset <code>dns_icassp_2020</code>, and this directory contains data loading classes, training, and inference scripts for this dataset:</p> <ul> <li>\ud83d\udcc4 <code>run.py</code>: The entry of the entire project, which can be used to train all models in the <code>dns_icassp_2020</code> directory.</li> <li>\ud83d\udcc4 <code>dataset_train.py</code>: The construction class of the training dataset.</li> <li>\ud83d\udcc4 <code>dataset_validation_dns_1.py</code>: The construction class of the first validation dataset.</li> <li>\ud83d\udcc4 <code>dataset_validation_dns_4_non_personalized.py</code>: The construction class of the second validation dataset.</li> </ul> <p><code>cirm_lstm/</code> contains the cIRM LSTM model for this dataset, including the structure and trainers for each model.</p> <ul> <li>\ud83d\udcc4 <code>&lt;exp_id&gt;.toml</code>: The training parameters for this model.</li> <li>\ud83d\udcc4 <code>trainer.py</code>: The trainer for this model, which contains the operations and operations to be executed in each training, validation and test round.</li> <li>\ud83d\udcc4 <code>model.py</code>: The structure of the current model.</li> <li>\ud83d\udcc4 <code>run.py</code> (optional): The entry of the current model, which can be used to train the current model. If this file is not present, the <code>run.py</code> file in the <code>recipes/dns_icassp_2020</code> directory will be used.</li> </ul>"},{"location":"getting_started/running_an_experiment/","title":"Running an experiment","text":"<p>AudioZEN adopts a <code>recipes/&lt;dataset&gt;/&lt;model&gt;</code> direcotry structure. To run an experiment of a model, we first need to enter a dataset direcotry, which will include a entry file <code>run.py</code> and some dataloaders dedicated to this dataset. For example, let us entry to the directory <code>recipes/dns_icassp_2020/</code>. The correspoding dataset is the ICASSP 2020 DNS Challenge dataset:</p> <pre><code>cd recipes/dns_icassp_2020\n</code></pre>"},{"location":"getting_started/running_an_experiment/#entry-file-runpy","title":"Entry file <code>run.py</code>","text":"<p>In each <code>&lt;dataset&gt;</code> directory, we have a entry file <code>run.py</code>, dataloaders, and some model direcotries. Then, we call this <code>run.py</code> script to run the experiment. For example, we can use the following command to train the <code>cirm_lstm</code> model using configurations in <code>baseline.toml</code>:</p> <pre><code>torchrun run.py -C cirm_lstm/baseline.toml -M train\n</code></pre> <p>Here, <code>torchrun</code> helps us to start multi-GPU training conveniently. <code>torchrun</code> isn't a magic, its just a python <code>console_entrypoint</code> added for convenience (check torchrun versus python -m torch.distributed.run).</p> <p><code>run.py</code> supports the following parameters:</p> <pre><code>usage: run.py [-h] -C CONFIGURATION [-M {train,validate,test,predict,finetune} [{train,validate,test,predict,finetune} ...]] [-R] [--ckpt_path CKPT_PATH]\nAudio-ZEN\n\noptions:\n  -h, --help            show this help message and exit\n-C CONFIGURATION, --configuration CONFIGURATION\n                        Configuration (*.toml).\n  -M {train,validate,test,predict,finetune} [{train,validate,test,predict,finetune} ...], --mode {train,validate,test,predict,finetune} [{train,validate,test,predict,finetune} ...]\nMode of the experiment.\n  -R, --resume          Resume the experiment from latest checkpoint.\n  --ckpt_path CKPT_PATH\n                        Checkpoint path for test. It can be 'best', 'latest', or a path to a checkpoint.\n</code></pre> <p>See more details in <code>recipes/dns_icassp_2020/run.py</code> and the configuration file <code>recipes/dns_icassp_2020/cirm_lstm/baseline.toml</code>.</p>"},{"location":"getting_started/running_an_experiment/#single-machine-multi-gpu-training","title":"Single-machine multi-GPU training","text":"<p>In most cases, we want to start an experiment on a single machine with multiple GPUs. Here, we show some examples for how to. First, let us use <code>baseline.toml</code> to train <code>cirm_lstm</code> with 2 GPUs on a single machine</p> <pre><code>torchrun\n    --standalone\n    --nnodes=1\n--nproc_per_node=2\nrun.py\n    --configuration cirm_lstm/baseline.toml\n    --mode train\n</code></pre> <p>Use baseline.toml to train cirm_lstm with 1 GPU on a single machine</p> <pre><code>torchrun\n    --standalone\n    --nnodes=1\n--nproc_per_node=1\nrun.py\n    --configuration cirm_lstm/baseline.toml\n    --mode train\n</code></pre> <p>Use <code>baseline.toml</code> to train cirm_lstm with 2 GPUs on a single machine, and resume training (using <code>-R</code> or <code>--resume</code>) from the last checkpoint:</p> <pre><code>torchrun\n    --standalone\n    --nnodes=1\n--nproc_per_node=2\nrun.py\n    -C cirm_lstm/baseline.toml\n    -M train\n    -R\n</code></pre> <p>In the case of running multiple experiments on a single machine, since the first experiment has occupied the default DistributedDataParallel (DDP) listening port 29500, we need to make sure that each instance (job) is setup on different ports to avoid port conflicts. Use <code>rdzv-endpoint=localhost:0</code> means to select a random unused port:</p> <pre><code>torchrun\n    --rdzv-backend=c10d\n    --rdzv-endpoint=localhost:0\n    --nnodes=1\n--nproc_per_node=2\nrun.py\n    -C cirm_lstm/baseline.toml\n    -M train\n</code></pre> <p>Using \"best\" epoch to test the model performance on the test dataset:</p> <pre><code>torchrun\n    --standalone\n    --nnodes=1\n--nproc_per_node=2\nrun.py\n    -C cirm_lstm/baseline.toml\n    -M test\n--ckpt_path best\n</code></pre> <p>Sequential train the model on the training dataset and test the model performance on the test dataset:</p> <pre><code>torchrun\n    --standalone\n    --nnodes=1\n--nproc_per_node=2\nrun.py\n    -C cirm_lstm/baseline.toml\n    -M train test\n--ckpt_path best\n</code></pre> <p>Important</p> <p>Before use <code>torchrun</code>, don't forget to use the environment variable <code>CUDA_VISIBLE_DEVICES</code> to control the GPU usage. For example, the following command will use the first and second GPUs:</p> <pre><code>export CUDA_VISIABLE_DEVICES=0,1\n</code></pre>"},{"location":"reference/FAQ/","title":"FAQ","text":""},{"location":"reference/FAQ/#torchdistributedbarrier-hangs-in-ddp","title":"<code>torch.distributed.barrier()</code> hangs in DDP","text":"<p>Use <code>model.module</code> instead of <code>model</code> during validation.</p> <p>Check https://discuss.pytorch.org/t/torch-distributed-barrier-hangs-in-ddp/114522/6 for more details.</p>"},{"location":"reference/changelog/","title":"Changelog","text":"<p><code>{include} ../../../CHANGELOG.md</code></p>"},{"location":"reference/conduct/","title":"Conduct","text":"<p><code>{include} ../../../CODE_OF_CONDUCT.md</code></p>"},{"location":"reference/contributing/","title":"Contributing","text":"<p><code>{include} ../../../CONTRIBUTING.md</code></p>"}]}